{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mollyperlich/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/mollyperlich/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Modeling using TITLE instead of content\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import collections, re\n",
    "from sklearn import preprocessing, cross_validation, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdate</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'If Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'A YouT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'Q.  I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   createdate                                           articles\n",
       "0  2019-07-04  {'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...\n",
       "1  2019-07-04  {'publishdate': '1/2/2018', 'content': 'If Pro...\n",
       "2  2019-07-04  {'publishdate': '1/2/2018', 'content': 'A YouT...\n",
       "3  2019-07-04  {'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...\n",
       "4  2019-07-04  {'publishdate': '1/2/2018', 'content': 'Q.  I ..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "data = pd.read_json('2.News_With_Labels.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdate</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'If Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'A YouT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-04</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'Q.  I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   createdate                                           articles\n",
       "0  2019-07-04  {'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...\n",
       "1  2019-07-04  {'publishdate': '1/2/2018', 'content': 'If Pro...\n",
       "2  2019-07-04  {'publishdate': '1/2/2018', 'content': 'A YouT...\n",
       "3  2019-07-04  {'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...\n",
       "4  2019-07-04  {'publishdate': '1/2/2018', 'content': 'Q.  I ..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to df\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in raw data: 48384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How an A.I. ‘Cat-and-Mouse Game’ Generates Bel...</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Activists’ Guide to 2018: The Best Defense Is ...</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logan Paul, YouTube Star, Says Posting Video o...</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to Manage Your Career</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adjusting Twitter’s Sensitivities</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title label      category\n",
       "0  How an A.I. ‘Cat-and-Mouse Game’ Generates Bel...   Buy    Technology\n",
       "1  Activists’ Guide to 2018: The Best Defense Is ...   Buy      Business\n",
       "2  Logan Paul, YouTube Star, Says Posting Video o...   Buy      Business\n",
       "3                          How to Manage Your Career   Buy  Business Day\n",
       "4                  Adjusting Twitter’s Sensitivities   Buy      Business"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df = pd.DataFrame([])\n",
    "\n",
    "for i in df['articles']:\n",
    "    stock_df = stock_df.append(pd.DataFrame({'title': i['title'], 'label': i['label'], 'category': i['category']}, index=[0]), ignore_index=True)\n",
    "\n",
    "print(f\"Number of words in raw data: {stock_df['title'].apply(lambda x: len(x.split(' '))).sum()}\") \n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of indexes for which column category is Technology\n",
    "indexNames = stock_df[ stock_df['category'] == 'Technology'].index\n",
    " \n",
    "stock_df.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get names of indexes for which column category is Business Day\n",
    "# indexNames2 = stock_df[ stock_df['category'] == 'Business Day'].index\n",
    " \n",
    "# stock_df.drop(indexNames2 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of indexes for which column category is Politics\n",
    "indexNames3 = stock_df[ stock_df['category'] == 'Politics'].index\n",
    " \n",
    "stock_df.drop(indexNames3 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Activists’ Guide to 2018: The Best Defense Is ...</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logan Paul, YouTube Star, Says Posting Video o...</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to Manage Your Career</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adjusting Twitter’s Sensitivities</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TV Content Wars: Too Many Characters Chasing N...</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title label      category\n",
       "1  Activists’ Guide to 2018: The Best Defense Is ...   Buy      Business\n",
       "2  Logan Paul, YouTube Star, Says Posting Video o...   Buy      Business\n",
       "3                          How to Manage Your Career   Buy  Business Day\n",
       "4                  Adjusting Twitter’s Sensitivities   Buy      Business\n",
       "5  TV Content Wars: Too Many Characters Chasing N...   Buy      Business"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep a count of how many you delete\n",
    "bad_rows = []\n",
    "for i in range(0,stock_df.shape[0]):\n",
    "    #if the number of characters is below 100\n",
    "    if len(stock_df['title'].iloc[i])<100:\n",
    "        bad_rows.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mollyperlich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Number of words AFTER clean up: 116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai catandmouse game generates believable fake ...</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>asked ces 2018 answered</td>\n",
       "      <td>Sell</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>ratio establishes twitter</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>social media gives women voice</td>\n",
       "      <td>Buy</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>selfdriving uber killed pedestrian arizona</td>\n",
       "      <td>Hold</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title label    category\n",
       "0    ai catandmouse game generates believable fake ...   Buy  Technology\n",
       "42                             asked ces 2018 answered  Sell  Technology\n",
       "273                          ratio establishes twitter   Buy  Technology\n",
       "279                     social media gives women voice   Buy  Technology\n",
       "535         selfdriving uber killed pedestrian arizona  Hold  Technology"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#clean up the text\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "#function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text\n",
    "    \n",
    "#apply function to each row in the dataframe\n",
    "stock_df['title'] = stock_df['title'].apply(clean_text)\n",
    "#number of words after clean up\n",
    "print(f\"Number of words AFTER clean up: {stock_df['title'].apply(lambda x: len(x.split(' '))).sum()}\") \n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_ = []\n",
    "for item in stock_df[\"title\"]:\n",
    "    if (item in content_):\n",
    "        continue\n",
    "    else:\n",
    "        content_.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = stock_df.title\n",
    "y = stock_df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ai': 1,\n",
       "         'catandmouse': 1,\n",
       "         'game': 1,\n",
       "         'generates': 1,\n",
       "         'believable': 1,\n",
       "         'fake': 1,\n",
       "         'photos': 1})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagsofwords = [ collections.Counter(re.findall(r'\\w+', txt))\n",
    "            for txt in content_]\n",
    "bagsofwords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['2018', 'access', 'across', 'ai', 'alexa', 'answered', 'answers', 'apples', 'arizona', 'asked', 'believable', 'best', 'breach', 'caregiver', 'carry', 'catandmouse', 'ces', 'consider', 'conversation', 'costs', 'could', 'data', 'deceptive', 'deep', 'device', 'disinformation', 'distracted', 'edition', 'establishes', 'evolving', 'facebook', 'fake', 'followers', 'friends', 'game', 'gave', 'generates', 'gift', 'gives', 'google', 'guide', 'hands', 'hate', 'holiday', 'home', 'house', 'india', 'information', 'internet', 'iphones', 'killed', 'leads', 'lost', 'made', 'make', 'makers', 'many', 'marriott', 'media', 'meet', 'minaj', 'mobs', 'much', 'murder', 'new', 'nicki', 'oceans', 'online', 'others', 'pedestrian', 'photos', 'post', 'protect', 'purge', 'questions', 'quiz', 'ratio', 'robot', 'selfdriving', 'selfies', 'siri', 'smart', 'social', 'speech', 'spot', 'spread', 'superintendents', 'take', 'technology', 'testimony', 'thing', 'travels', 'trump', 'twitter', 'uber', 'us', 'users', 'voice', 'vs', 'weaponized', 'weekly', 'whatsapp', 'women', 'zora']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(content_) \n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(content_)\n",
    "# summarize\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.idf_)\n",
    "# encode document\n",
    "#vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "#print(vector.shape)\n",
    "#print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 104)\n",
      "[[0.45445063 0.         0.         0.         0.         0.51428741\n",
      "  0.         0.         0.         0.51428741 0.         0.\n",
      "  0.         0.         0.         0.         0.51428741 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform([content_[1]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words with Keras\n",
    "The Keras Python library for deep learning also provides tools for encoding text using the bag-of words-model in the Tokenizer class.\n",
    "As above, the encoder must be trained on source documents and then can be used to encode training data, test data and any other data in the future. The API also has the benefit of performing basic tokenization prior to encoding the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tk = Tokenizer(num_words=2)\n",
    "texts = content_\n",
    "tk.fit_on_texts(texts)\n",
    "# # create the tokenizer\n",
    "# t = Tokenizer()\n",
    "# # fit the tokenizer on the documents\n",
    "# t.fit_on_texts(content_)\n",
    "# # summarize what was learned\n",
    "# # print(t.word_counts)\n",
    "# # print(t.document_count)\n",
    "# # print(t.word_index)\n",
    "# # print(t.word_docs)\n",
    "# # integer encode documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [1],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [1],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [1],\n",
       " [1],\n",
       " []]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'facebook': 1,\n",
       " 'ai': 2,\n",
       " '2018': 3,\n",
       " 'twitter': 4,\n",
       " 'uber': 5,\n",
       " 'gave': 6,\n",
       " 'robot': 7,\n",
       " 'vs': 8,\n",
       " 'meet': 9,\n",
       " 'us': 10,\n",
       " 'catandmouse': 11,\n",
       " 'game': 12,\n",
       " 'generates': 13,\n",
       " 'believable': 14,\n",
       " 'fake': 15,\n",
       " 'photos': 16,\n",
       " 'asked': 17,\n",
       " 'ces': 18,\n",
       " 'answered': 19,\n",
       " 'ratio': 20,\n",
       " 'establishes': 21,\n",
       " 'social': 22,\n",
       " 'media': 23,\n",
       " 'gives': 24,\n",
       " 'women': 25,\n",
       " 'voice': 26,\n",
       " 'selfdriving': 27,\n",
       " 'killed': 28,\n",
       " 'pedestrian': 29,\n",
       " 'arizona': 30,\n",
       " 'weekly': 31,\n",
       " 'edition': 32,\n",
       " 'device': 33,\n",
       " 'makers': 34,\n",
       " 'deep': 35,\n",
       " 'access': 36,\n",
       " 'data': 37,\n",
       " 'users': 38,\n",
       " 'friends': 39,\n",
       " 'internet': 40,\n",
       " 'travels': 41,\n",
       " 'across': 42,\n",
       " 'oceans': 43,\n",
       " 'could': 44,\n",
       " 'weaponized': 45,\n",
       " 'spread': 46,\n",
       " 'disinformation': 47,\n",
       " 'purge': 48,\n",
       " 'many': 49,\n",
       " 'followers': 50,\n",
       " 'trump': 51,\n",
       " 'nicki': 52,\n",
       " 'minaj': 53,\n",
       " 'others': 54,\n",
       " 'lost': 55,\n",
       " 'hands': 56,\n",
       " 'evolving': 57,\n",
       " 'alexa': 58,\n",
       " 'siri': 59,\n",
       " 'google': 60,\n",
       " 'carry': 61,\n",
       " 'conversation': 62,\n",
       " 'best': 63,\n",
       " 'spot': 64,\n",
       " 'deceptive': 65,\n",
       " 'post': 66,\n",
       " 'whatsapp': 67,\n",
       " 'leads': 68,\n",
       " 'mobs': 69,\n",
       " 'murder': 70,\n",
       " 'india': 71,\n",
       " 'zora': 72,\n",
       " 'caregiver': 73,\n",
       " 'holiday': 74,\n",
       " 'gift': 75,\n",
       " 'guide': 76,\n",
       " 'technology': 77,\n",
       " 'make': 78,\n",
       " 'house': 79,\n",
       " 'smart': 80,\n",
       " 'home': 81,\n",
       " 'marriott': 82,\n",
       " 'breach': 83,\n",
       " 'protect': 84,\n",
       " 'information': 85,\n",
       " 'online': 86,\n",
       " 'answers': 87,\n",
       " 'questions': 88,\n",
       " 'apples': 89,\n",
       " 'new': 90,\n",
       " 'iphones': 91,\n",
       " 'distracted': 92,\n",
       " 'made': 93,\n",
       " 'selfies': 94,\n",
       " 'thing': 95,\n",
       " 'consider': 96,\n",
       " 'hate': 97,\n",
       " 'speech': 98,\n",
       " 'take': 99,\n",
       " 'quiz': 100,\n",
       " 'testimony': 101,\n",
       " 'much': 102,\n",
       " 'costs': 103,\n",
       " 'superintendents': 104}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs = tk.texts_to_matrix(texts, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_docs))\n",
    "# print(len(text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(content_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.4\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Buy       0.50      0.67      0.57         3\n",
      "       Hold       0.00      0.00      0.00         2\n",
      "       Sell       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.30      0.40      0.34         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mollyperlich/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/mollyperlich/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#logisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Buy       0.60      1.00      0.75         3\n",
      "       Hold       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.36      0.60      0.45         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mollyperlich/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "r_forests = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "               ])\n",
    "r_forests.fit(X_train, y_train)\n",
    "\n",
    "y_pred = r_forests.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14 samples, validate on 2 samples\n",
      "Epoch 1/4\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.0988 - acc: 0.2857 - val_loss: 1.1154 - val_acc: 0.0000e+00\n",
      "Epoch 2/4\n",
      "14/14 [==============================] - 0s 472us/step - loss: 1.0630 - acc: 0.3571 - val_loss: 1.0995 - val_acc: 0.0000e+00\n",
      "Epoch 3/4\n",
      "14/14 [==============================] - 0s 511us/step - loss: 1.0097 - acc: 0.7857 - val_loss: 1.0835 - val_acc: 0.0000e+00\n",
      "Epoch 4/4\n",
      "14/14 [==============================] - 0s 492us/step - loss: 0.9763 - acc: 0.8571 - val_loss: 1.0680 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#Keras \n",
    "\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_size = int(len(stock_df) * .7)\n",
    "train_posts = stock_df['title'][:train_size]\n",
    "train_tags = stock_df['label'][:train_size]\n",
    "\n",
    "test_posts = stock_df['title'][train_size:]\n",
    "test_tags = stock_df['label'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 176us/step\n",
      "Test accuracy: 0.2857142984867096\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
