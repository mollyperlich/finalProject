{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdate</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'If Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'A YouT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>{'publishdate': '1/2/2018', 'content': 'Q.  I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   createdate                                           articles\n",
       "0  2019-07-03  {'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...\n",
       "1  2019-07-03  {'publishdate': '1/2/2018', 'content': 'If Pro...\n",
       "2  2019-07-03  {'publishdate': '1/2/2018', 'content': 'A YouT...\n",
       "3  2019-07-03  {'publishdate': '1/2/2018', 'content': '\n",
       "\n",
       "\n",
       "Mor...\n",
       "4  2019-07-03  {'publishdate': '1/2/2018', 'content': 'Q.  I ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "df = pd.read_json('2.News_With_Labels.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in raw data: 4827973\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\nMore on NYTimes.com\\n\\n\\n\\n\\n</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If Procter &amp; Gamble were sponsoring a 2018 adv...</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A YouTube star with millions of followers apol...</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\nMore on NYTimes.com\\n\\n\\n\\n\\n</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q.  I see posts in my Twitter feed that have “...</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label\n",
       "0                \\n\\n\\nMore on NYTimes.com\\n\\n\\n\\n\\n   Buy\n",
       "1  If Procter & Gamble were sponsoring a 2018 adv...   Buy\n",
       "2  A YouTube star with millions of followers apol...   Buy\n",
       "3                \\n\\n\\nMore on NYTimes.com\\n\\n\\n\\n\\n   Buy\n",
       "4  Q.  I see posts in my Twitter feed that have “...   Buy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#format data into pandas dataframe\n",
    "stock_df = pd.DataFrame([])\n",
    "\n",
    "for i in df['articles']:\n",
    "    #stock_df = stock_df.append(pd.DataFrame({'publish_date': i['publishdate'], 'content': i['content'], 'label': i['label']}, index=[0]), ignore_index=True)\n",
    "    stock_df = stock_df.append(pd.DataFrame({'content': i['content'], 'label': i['label']}, index=[0]), ignore_index=True)\n",
    "\n",
    "#number of characters\n",
    "print(f\"Number of words in raw data: {stock_df['content'].apply(lambda x: len(x.split(' '))).sum()}\") \n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's take a look at the label spread\n",
    "plt.figure(figsize=(10,4))\n",
    "stock_df.label.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spread looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing:\n",
    "Remove stop words, change text to lower case, remove punctuation, remove bad characters, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashcr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words AFTER clean up: 2906051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nytimescom</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>procter gamble sponsoring 2018 advertisement a...</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youtube star millions followers apologized mon...</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nytimescom</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q see posts twitter feed sensitive content lab...</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label\n",
       "0                                         nytimescom   Buy\n",
       "1  procter gamble sponsoring 2018 advertisement a...   Buy\n",
       "2  youtube star millions followers apologized mon...   Buy\n",
       "3                                         nytimescom   Buy\n",
       "4  q see posts twitter feed sensitive content lab...   Buy"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#clean up the text\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "#function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text\n",
    "    \n",
    "#apply function to each row in the dataframe\n",
    "stock_df['content'] = stock_df['content'].apply(clean_text)\n",
    "#number of words after clean up\n",
    "print(f\"Number of words AFTER clean up: {stock_df['content'].apply(lambda x: len(x.split(' '))).sum()}\") \n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to cut down the amount of words/characters significantly! (From 5 million to 3 million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nytimescom'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df['content'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice some of the rows have little to no content\n",
    "#delete these rows to avoid error in the model\n",
    "\n",
    "#keep a count of how many you delete\n",
    "bad_rows = []\n",
    "for i in range(0,stock_df.shape[0]):\n",
    "    #if the number of characters is below 100\n",
    "    if len(stock_df['content'].iloc[i])<100:\n",
    "        bad_rows.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4848, 2)\n",
      "                                             content label\n",
      "0  procter gamble sponsoring 2018 advertisement a...   Buy\n",
      "1  youtube star millions followers apologized mon...   Buy\n",
      "2  q see posts twitter feed sensitive content lab...   Buy\n",
      "3  golden age television programming heading gris...   Buy\n",
      "4  please take seatsit time year annual closing d...   Buy\n"
     ]
    }
   ],
   "source": [
    "# #drop the rows we found to have fewer than 100 characters\n",
    "stock_df = stock_df.drop(bad_rows)\n",
    "#reset index\n",
    "stock_df = stock_df.reset_index(drop=True)\n",
    "print(stock_df.shape)\n",
    "print(stock_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data set\n",
    "X = stock_df.content\n",
    "y = stock_df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.3917525773195876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Buy       0.67      0.01      0.01       292\n",
      "        Hold       0.39      1.00      0.56       376\n",
      "        Sell       0.43      0.01      0.02       302\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       970\n",
      "   macro avg       0.50      0.34      0.20       970\n",
      "weighted avg       0.49      0.39      0.23       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#convert to a matrix of token counts (CountVectorizer)\n",
    "#then transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINES (REGARDED AS ONE OF THE BEST TEXT CLASSIFIERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashcr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.38969072164948454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Buy       0.33      0.28      0.30       292\n",
      "        Hold       0.41      0.58      0.48       376\n",
      "        Sell       0.40      0.26      0.32       302\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       970\n",
      "   macro avg       0.38      0.37      0.37       970\n",
      "weighted avg       0.38      0.39      0.38       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashcr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ashcr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.37216494845360826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Buy       0.31      0.27      0.29       292\n",
      "        Hold       0.42      0.49      0.45       376\n",
      "        Sell       0.36      0.32      0.34       302\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       970\n",
      "   macro avg       0.36      0.36      0.36       970\n",
      "weighted avg       0.37      0.37      0.37       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.3917525773195876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Buy       0.34      0.21      0.25       292\n",
      "        Hold       0.40      0.72      0.52       376\n",
      "        Sell       0.41      0.17      0.24       302\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       970\n",
      "   macro avg       0.38      0.36      0.34       970\n",
      "weighted avg       0.39      0.39      0.35       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "r_forests = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "               ])\n",
    "r_forests.fit(X_train, y_train)\n",
    "\n",
    "y_pred = r_forests.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_size = int(len(stock_df) * .7)\n",
    "train_posts = stock_df['content'][:train_size]\n",
    "train_tags = stock_df['label'][:train_size]\n",
    "\n",
    "test_posts = stock_df['content'][train_size:]\n",
    "test_tags = stock_df['label'][train_size:]\n",
    "\n",
    "max_words = 1500\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
