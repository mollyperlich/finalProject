{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdate</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-07</td>\n",
       "      <td>{'publishdate': '2018-01-02', 'content': '\n",
       "\n",
       "\n",
       "M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-07</td>\n",
       "      <td>{'publishdate': '2018-01-02', 'content': 'If P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-07</td>\n",
       "      <td>{'publishdate': '2018-01-02', 'content': 'A Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-07</td>\n",
       "      <td>{'publishdate': '2018-01-02', 'content': '\n",
       "\n",
       "\n",
       "M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-07</td>\n",
       "      <td>{'publishdate': '2018-01-02', 'content': 'Q.  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   createdate                                           articles\n",
       "0  2019-07-07  {'publishdate': '2018-01-02', 'content': '\n",
       "\n",
       "\n",
       "M...\n",
       "1  2019-07-07  {'publishdate': '2018-01-02', 'content': 'If P...\n",
       "2  2019-07-07  {'publishdate': '2018-01-02', 'content': 'A Yo...\n",
       "3  2019-07-07  {'publishdate': '2018-01-02', 'content': '\n",
       "\n",
       "\n",
       "M...\n",
       "4  2019-07-07  {'publishdate': '2018-01-02', 'content': 'Q.  ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "df = pd.read_json('3.News_With_Labels.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in raw data: 4827973\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\nMore on NYTimes.com\\n\\n\\n\\n\\n</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If Procter &amp; Gamble were sponsoring a 2018 adv...</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A YouTube star with millions of followers apol...</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\nMore on NYTimes.com\\n\\n\\n\\n\\n</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q.  I see posts in my Twitter feed that have “...</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label\n",
       "0                \\n\\n\\nMore on NYTimes.com\\n\\n\\n\\n\\n  Hold\n",
       "1  If Procter & Gamble were sponsoring a 2018 adv...  Hold\n",
       "2  A YouTube star with millions of followers apol...  Hold\n",
       "3                \\n\\n\\nMore on NYTimes.com\\n\\n\\n\\n\\n  Hold\n",
       "4  Q.  I see posts in my Twitter feed that have “...  Hold"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#format data into pandas dataframe\n",
    "stock_df = pd.DataFrame([])\n",
    "\n",
    "for i in df['articles']:\n",
    "    #stock_df = stock_df.append(pd.DataFrame({'publish_date': i['publishdate'], 'content': i['content'], 'label': i['label']}, index=[0]), ignore_index=True)\n",
    "    stock_df = stock_df.append(pd.DataFrame({'content': i['content'], 'label': i['label']}, index=[0]), ignore_index=True)\n",
    "\n",
    "#number of characters\n",
    "print(f\"Number of words in raw data: {stock_df['content'].apply(lambda x: len(x.split(' '))).sum()}\") \n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's take a look at the label spread\n",
    "plt.figure(figsize=(10,4))\n",
    "stock_df.label.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spread looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing:\n",
    "Remove stop words, change text to lower case, remove punctuation, remove bad characters, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashcr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words AFTER clean up: 2906051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nytimescom</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>procter gamble sponsoring 2018 advertisement a...</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youtube star millions followers apologized mon...</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nytimescom</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q see posts twitter feed sensitive content lab...</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label\n",
       "0                                         nytimescom  Hold\n",
       "1  procter gamble sponsoring 2018 advertisement a...  Hold\n",
       "2  youtube star millions followers apologized mon...  Hold\n",
       "3                                         nytimescom  Hold\n",
       "4  q see posts twitter feed sensitive content lab...  Hold"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#clean up the text\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "#function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text\n",
    "    \n",
    "#apply function to each row in the dataframe\n",
    "stock_df['content'] = stock_df['content'].apply(clean_text)\n",
    "#number of words after clean up\n",
    "print(f\"Number of words AFTER clean up: {stock_df['content'].apply(lambda x: len(x.split(' '))).sum()}\") \n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to cut down the amount of words/characters significantly! (From 5 million to 3 million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice some of the rows have little to no content\n",
    "#delete these rows to avoid error in the model\n",
    "\n",
    "#keep a count of how many you delete\n",
    "bad_rows = []\n",
    "for i in range(0,stock_df.shape[0]):\n",
    "    #if the number of characters is below 100\n",
    "    if len(stock_df['content'].iloc[i])<100:\n",
    "        bad_rows.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4848, 2)\n",
      "                                             content label\n",
      "0  procter gamble sponsoring 2018 advertisement a...  Hold\n",
      "1  youtube star millions followers apologized mon...  Hold\n",
      "2  q see posts twitter feed sensitive content lab...  Hold\n",
      "3  golden age television programming heading gris...  Hold\n",
      "4  please take seatsit time year annual closing d...  Hold\n"
     ]
    }
   ],
   "source": [
    "# #drop the rows we found to have fewer than 100 characters\n",
    "stock_df = stock_df.drop(bad_rows)\n",
    "#reset index\n",
    "stock_df = stock_df.reset_index(drop=True)\n",
    "print(stock_df.shape)\n",
    "print(stock_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data set\n",
    "X = stock_df.content\n",
    "y = stock_df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6309278350515464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Buy       0.00      0.00      0.00       162\n",
      "        Hold       0.63      1.00      0.77       612\n",
      "        Sell       0.00      0.00      0.00       196\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       970\n",
      "   macro avg       0.21      0.33      0.26       970\n",
      "weighted avg       0.40      0.63      0.49       970\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashcr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#convert to a matrix of token counts (CountVectorizer)\n",
    "#then transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINES (REGARDED AS ONE OF THE BEST TEXT CLASSIFIERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6329896907216495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Buy       0.00      0.00      0.00       162\n",
      "        Hold       0.63      1.00      0.78       612\n",
      "        Sell       0.50      0.01      0.02       196\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       970\n",
      "   macro avg       0.38      0.34      0.27       970\n",
      "weighted avg       0.50      0.63      0.49       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashcr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ashcr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5783505154639176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Buy       0.18      0.10      0.13       162\n",
      "        Hold       0.67      0.83      0.74       612\n",
      "        Sell       0.31      0.20      0.24       196\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       970\n",
      "   macro avg       0.39      0.37      0.37       970\n",
      "weighted avg       0.52      0.58      0.54       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6309278350515464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Buy       0.25      0.01      0.01       162\n",
      "        Hold       0.64      1.00      0.78       612\n",
      "        Sell       0.29      0.01      0.02       196\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       970\n",
      "   macro avg       0.39      0.34      0.27       970\n",
      "weighted avg       0.50      0.63      0.50       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "r_forests = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "               ])\n",
    "r_forests.fit(X_train, y_train)\n",
    "\n",
    "y_pred = r_forests.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3102 samples, validate on 776 samples\n",
      "Epoch 1/10\n",
      "3102/3102 [==============================] - 5s 2ms/step - loss: 0.9715 - acc: 0.6241 - val_loss: 1.2574 - val_acc: 0.3621\n",
      "Epoch 2/10\n",
      "3102/3102 [==============================] - 1s 410us/step - loss: 0.8016 - acc: 0.6615 - val_loss: 1.3601 - val_acc: 0.3673\n",
      "Epoch 3/10\n",
      "3102/3102 [==============================] - 1s 380us/step - loss: 0.6533 - acc: 0.7311 - val_loss: 1.4440 - val_acc: 0.3737\n",
      "Epoch 4/10\n",
      "3102/3102 [==============================] - 1s 378us/step - loss: 0.4920 - acc: 0.8169 - val_loss: 1.6907 - val_acc: 0.3724\n",
      "Epoch 5/10\n",
      "3102/3102 [==============================] - 1s 383us/step - loss: 0.3565 - acc: 0.8872 - val_loss: 1.6993 - val_acc: 0.3686\n",
      "Epoch 6/10\n",
      "3102/3102 [==============================] - 1s 427us/step - loss: 0.2395 - acc: 0.9362 - val_loss: 1.9915 - val_acc: 0.3763\n",
      "Epoch 7/10\n",
      "3102/3102 [==============================] - ETA: 0s - loss: 0.1658 - acc: 0.9607- ETA: 0s - loss: 0.1 - 1s 404us/step - loss: 0.1658 - acc: 0.9607 - val_loss: 2.2322 - val_acc: 0.3789\n",
      "Epoch 8/10\n",
      "3102/3102 [==============================] - 1s 409us/step - loss: 0.1221 - acc: 0.9765 - val_loss: 2.0620 - val_acc: 0.3827\n",
      "Epoch 9/10\n",
      "3102/3102 [==============================] - 1s 395us/step - loss: 0.0875 - acc: 0.9836 - val_loss: 2.3171 - val_acc: 0.3737\n",
      "Epoch 10/10\n",
      "3102/3102 [==============================] - 1s 387us/step - loss: 0.0712 - acc: 0.9868 - val_loss: 2.2818 - val_acc: 0.3724\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_size = int(len(stock_df) * .8)\n",
    "train_posts = stock_df['content'][:train_size]\n",
    "train_tags = stock_df['label'][:train_size]\n",
    "\n",
    "test_posts = stock_df['content'][train_size:]\n",
    "test_tags = stock_df['label'][train_size:]\n",
    "\n",
    "max_words = 2000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "#model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Dense(200, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970/970 [==============================] - 0s 164us/step\n",
      "Test accuracy: 0.6453608248651642\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
